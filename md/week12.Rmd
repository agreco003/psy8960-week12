---
title: "week12.Rmd"
author: "Anthony Greco"
date: "2023-04-15"
output: html_document
---
Dataset Creation - Commented out as instructed
```{r Script Settings and Resources, include=TRUE} 
#setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
#library(tidyverse)
#library(RedditExtractoR)
```
```{r Data Import and Cleaning, include=TRUE}
# io_links <- find_thread_urls(
#   subreddit = "IOPsychology", 
#   period = "year"
# )
# io_dat <- get_thread_content(io_links$url)
# view(io_dat$threads)
#
# week12_tbl <- tibble(io_dat$threads, stringsAsFactors = FALSE) %>%
#   select(upvotes, title)
# 
# write_csv(week12_tbl, file = "../data/week12.csv")
```

Natural Language Processing (NLP)
```{r Script Settings and Resources, include=TRUE}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(tidyverse)
library(tm)
library(qdap) #had to install JavaScript to work properly
library(textstem)
library(RWeka)
library(topicmodels)
library(tidytext)
```

```{r Data Import and Cleaning, include=TRUE}
week12_tbl <- read_csv(file = "../data/week12.csv")
io_corpus_original <- VCorpus(VectorSource(week12_tbl$title))
io_corpus <- io_corpus_original %>% 
  tm_map(content_transformer(replace_abbreviation)) %>% #included steps below to homogenize word forms
  tm_map(content_transformer(replace_contraction)) %>%
  tm_map(content_transformer(str_to_lower)) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>% 
  tm_map(removeWords, c(stopwords("en"),
                        "io", "io psychology", "iopsychology",  "riopsychology", "io psych", "iopsych",
                        "industrial organizational psychology", "industrial organisational psychology",
                        "industrial organizational psych", "industrial organisational psych"
  )) %>% #removed variations of I/O psychology. Punctuation already removed previously so not needed here
  tm_map(stripWhitespace) %>%
  lemmatize_words() %>% #used a dictionary to further homogenize word forms
  tm_map(content_transformer(rm_white_lead)) #found the occasional leading whitespace in cleaned corpus examples annoying

compare_them <- function(x, y) { 
  index <- sample(1:length(x),1)
  print(x[[index]]$content)
  print(y[[index]]$content)
  }
compare_them(io_corpus_original, io_corpus)

myTokenizer <- function(x) { 
  NGramTokenizer(x, Weka_control(min=1, max=2)) } #set min and max to 2 to force bigrams (2-word sequence). Can set ot any limit but project called for 2.

io_dtm <- DocumentTermMatrix(io_corpus, control = list(tokenize = myTokenizer))
io_dtm$dimnames$Terms
test_matrix <- as.matrix(io_dtm)
empty <- tibble(rowSums(test_matrix))

io_slim_dtm <- removeSparseTerms(io_dtm, .9987) #cannot get a 2:1 -> 3:1 ratio of docs to variables. Jumps from 729:2875 #.9986282579
  
io_slim_dtm$dimnames$Terms
tokenCounts <- apply(io_slim_dtm, 1, sum)
io_slim_dtm <- io_slim_dtm[tokenCounts > 0, ]
as.matrix(io_slim_dtm)
```

```{r Analysis, include=TRUE}
lda_result <- LDA(io_dtm, 
                  k = 10,
                  method = "Gibbs")

# doc_id = lda_result@documents 
# original = week12_tbl$title
# topic = 
# probability = tidy(lda_result, matrix="beta")

# topics_tbl <- tibble(doc_id, original, topic)
```