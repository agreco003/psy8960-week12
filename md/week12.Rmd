---
title: "week12.Rmd"
author: "Anthony Greco"
date: "2023-04-15"
output: html_document
---
Dataset Creation - Commented out as instructed
```{r Script Settings and Resources, include=TRUE} 
# setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# library(tidyverse)
# library(RedditExtractoR)
```

```{r Data Import and Cleaning, include=TRUE}
# io_links <- find_thread_urls(
#   subreddit = "IOPsychology",
#   period = "year"
# )
# io_dat <- get_thread_content(io_links$url) #take a while to run but works!
# view(io_dat$threads)
# 
# week12_tbl <- tibble(io_dat$threads, stringsAsFactors = FALSE) %>%
#   select(upvotes, title)
# 
# write_csv(week12_tbl, file = "../data/week12.csv")
```

Natural Language Processing (NLP)
```{r Script Settings and Resources, include=TRUE}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(tidyverse)
library(tm)
library(qdap) #had to install JavaScript to work properly
library(textstem)
library(RWeka)
library(topicmodels)
library(tidytext)
library(doParallel)
library(ldatuning)
library(wordcloud)
```

```{r Data Import and Cleaning, include=TRUE}
week12_tbl <- read_csv(file = "../data/week12.csv")
io_corpus_original <- VCorpus(VectorSource(week12_tbl$title))
io_corpus <- io_corpus_original %>% 
  tm_map(content_transformer(replace_contraction)) %>% # contractions replaced
  tm_map(content_transformer(replace_abbreviation)) %>% # abbreviations replaced
  tm_map(content_transformer(replace_number)) %>% #numbers replaced but retained in case they are meaningful
  tm_map(content_transformer(str_to_lower)) %>% #all lower case
  tm_map(removePunctuation) %>% #remove punctuation
  tm_map(removeWords, c(stopwords("en"), "io psychology", "iopsychology",  "riopsychology", "i o psychology", "i o", "io"
                         #"industrial organizational psychology", "industrial organisational psychology",
                         #"industrial organizational psych", "industrial organisational psych"
                        )) %>% #removed variations of I/O psychology. Punctuation already removed previously so not needed here. removed because it trailed all & characters. 
  tm_map(stripWhitespace) %>%
  lemmatize_words() %>% #used a preset dictionary to homogenize word forms 
  tm_filter(FUN = function(x) { return(nchar(stripWhitespace(x$content)[[1]]) > 1) }) #filters 2 cases, comment out to find empty rows and rerun code chunk below

compare_them <- function(x, y) { 
  index <- sample(1:length(x),1) #set an index
  print(x[[index]]$content) #print content of index case x
  print(y[[index]]$content) #print content of index case y
  }
compare_them(io_corpus_original, io_corpus) #run it as many times you like for checks

myTokenizer <- function(x) { 
  NGramTokenizer(x, Weka_control(min=1, max=2)) } #set max to 2 to enable bigrams (2-word sequences). Bigrams include unigrams! 

io_dtm <- DocumentTermMatrix(io_corpus, control = list(tokenize = myTokenizer)) #create a DTM

# Check for empty values (0s)
# io_dtm$dimnames$Terms 
# test_matrix <- as.matrix(io_dtm)
# test_tbl <- tibble(count = rowSums(test_matrix), tidy(io_corpus_original))
# View(test_tbl) #check for 0 in terms - 2 missing
# # content(io_corpus_original[[455]])
# # content(io_corpus_original[[524]])

io_slim_dtm <- removeSparseTerms(io_dtm, .997) #2.1 n/k ratio (n cases = 729, k variables = 346)

```

```{r Analysis, include=TRUE}
# Identifying number of topics
local_cluster <- makeCluster(7)
registerDoParallel(local_cluster) #Run across multiple cores! Faster.
tuning <- FindTopicsNumber(io_dtm,
                           topics = seq(5, 15, by = 1), #originally set between 5 and 30 by 5, then 5 and 15 by 1, and then this final restricted range, looking for alignment across models
                           metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"), 
                           verbose = T,
                           control = list(seed = 50))
FindTopicsNumber_plot(tuning) #look for convergence between, similar to a PCA
stopCluster(local_cluster) 
registerDoSEQ() #closes clusters

#LDA with 8 topics, 7 or 9 would also be reasonable
lda_results <- LDA(io_dtm, 
                  k = 8,
                  method = "Gibbs",
                  control = list(seed = 25))

lda_betas <- tidy(lda_results, matrix="beta") %>% 
  group_by(topic) %>%
  slice_max(n = 10, beta) %>%
  arrange(topic, -beta)
lda_betas # probability that word(s) belong in a topic

lda_gammas <- tidy(lda_results, matrix="gamma") %>% 
  group_by(document) %>% 
  slice_max(n = 1, gamma, with_ties = FALSE) %>% #highest probability only, removes ties
  mutate(doc_id = as.numeric(document)) %>%
  arrange(doc_id)
lda_gammas # probabilities of document containing the topic, less than 729 will require creative table building

# Create a table to interpret results
topics_tbl <- tidy(io_corpus_original) %>%
  right_join(lda_gammas, by = join_by(id == document)) %>%
  mutate(original = text, probability = gamma) %>%
  select(doc_id, original, topic, probability)
``` 
#Analysis Questions: 
##1. Using the beta matrix alone, what topics would you conclude your final topic list maps onto? (e.g., topic 1, 2, 3â€¦n each reflect what substantive topic construct? Use your best judgment.)
Topic 1: Recommendations for entering the field (job, recommendation, help, undergrad, internship, interested, field, best)
Topic 2: Content of IO psychology (people, performance, data, read, tests, jobs)
Topic 3: Tasks of IO psychologists (career, development, assessment, learn)
Topic 4: Settings of IO psychologists (company, phd, field, siop, conference, recent)
Topic 5: Recurring discussions about topics (discussion, biweekly discussion, discussion reading, reading think)
Topic 6: Advice about the working in the field (experience, advice, anyone, questions, find, looking)
Topic 7: Time-bound questions (question, two-thousand, twenty, twenty-two)
Topic 8: Materials for IO content (leadership, resources, consulting, use, theory, article, good, know)

##2. Look at the original text of documents with the highest and lowest probabilities assigned to each document. Do your topic names derived from your interpretation of the beta matrix conceptually match with the content of the original posts? What kind of validity evidence does your answer to this question represent?
The topic names derived are just okay -- there is substantial overlap between topic types. This suggests limited face validity of our topic names, first and foremost. Further, it demonstrates limited construct validity, particularly discriminant validity, if one expected distinct topics to emerge from these Reddit titles.   

``` {r Visualization, include = TRUE}
wordcloud_data = as.matrix(io_dtm)
wordCounts <- colSums(wordcloud_data)
wordNames <- colnames(wordcloud_data)
wordcloud_tbl <- (tibble(wordNames, wordCounts)) #combined to a new table, because word cloud was generating duplicate terms otherwise (i.e discussion reading appeared twice)
colorset <- brewer.pal(7, "Dark2")
wordcloud(
	  words = wordcloud_tbl$wordNames,
		freq = wordcloud_tbl$wordCounts,
		max.words = 50,
		min.freq=2,
		scale=c(2,.3), #made print scale smaller to fit in more windows. Would remove words if it didn't fit
		colors=colorset) 
```
#Wordcloud interpretation:
Across all documents, the term, "job" appears most frequently, and represents its own topic. Another topic includes a series of frequently occurring terms regarding recurring discussions (biweekly, discussions, think, reading). A third category appears to focus on the tasks required of an IO psychologist (psych, work, research). Yet another topic appears involved time-bound questions, with numbers occurring most often (thousand twenty, two-thousand). A fifth topic seems to center on the quality (i.e best) of contexts for training and working as an IO psychologist, with IO programs (school, graduate, degree, phd), and applied settings (management, company, internship, consulting) appearing together. The 6th category searches for opinions, as "think" appears as a frequently occurring term. Of course, this interpretation should be taken with caution, as wordclouds offer limited explanatory power. This one, in particular, does not represent all 8 categories identified previously, is limited to only the 50 most frequently occurring words in the dataset, skewing our interpretation. 

``` {r Publication, include = TRUE} 
final_week12_tbl <- mutate(week12_tbl, doc_id = as.numeric(row.names(week12_tbl))) #added doc_id matching in multiple tibbles
final_tbl <- right_join(topics_tbl, final_week12_tbl, by = "doc_id") %>% #retains all case numbers, even missing values. Check title vs original column to ensure it worked correctly
  select(-original) %>% #2 missing titles in topics tibble
  mutate(topic = factor(topic, ordered = FALSE)) %>%
  rename(original = title) %>% #creating unordered factors for lm analysis later, renames titles column so all titles are present
  arrange(doc_id)

# Does topic predict upvotes?       
model <- lm(upvotes ~ topic, data = final_tbl)
summary(model)
```
#Final results interpretation:
Unfortunately, it seems our categories are not predictive of upvotes received. Though our intercept is significant, the model results in a very, very small r-squared value (~.005), suggesting our categories do not explain much of the variance in upvotes. 