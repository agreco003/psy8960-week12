---
title: "week12.Rmd"
author: "Anthony Greco"
date: "2023-04-15"
output: html_document
---
Dataset Creation - Commented out as instructed
```{r Script Settings and Resources, include=TRUE} 
# setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# library(tidyverse)
# library(RedditExtractoR)
```

```{r Data Import and Cleaning, include=TRUE}
# io_links <- find_thread_urls(
#   subreddit = "IOPsychology",
#   period = "year"
# )
# io_dat <- get_thread_content(io_links$url) #take a while to run but works!
# view(io_dat$threads)
# 
# week12_tbl <- tibble(io_dat$threads, stringsAsFactors = FALSE) %>%
#   select(upvotes, title)
# 
# write_csv(week12_tbl, file = "../data/week12.csv")
```

Natural Language Processing (NLP)
```{r Script Settings and Resources, include=TRUE}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(tidyverse)
library(tm)
library(qdap) #had to install JavaScript to work properly
library(textstem)
library(RWeka)
library(topicmodels)
library(tidytext)
library(doParallel)
library(ldatuning)
library(wordcloud)
```

```{r Data Import and Cleaning, include=TRUE}
week12_tbl <- read_csv(file = "../data/week12.csv")
io_corpus_original <- VCorpus(VectorSource(week12_tbl$title))
io_corpus <- io_corpus_original %>% 
  tm_map(content_transformer(replace_contraction)) %>% # contractions replaced
  tm_map(content_transformer(replace_abbreviation)) %>% # abbreviations replaced
  tm_map(content_transformer(replace_number)) %>% #numbers replaced but retained in case they are meaningful
  tm_map(content_transformer(str_to_lower)) %>% #all lower case
  tm_map(removePunctuation) %>% #remove punctuation
  tm_map(removeWords, c(stopwords("en"), "io psychology", "iopsychology",  "riopsychology", "i o psychology", "i o", "io"
                         #"industrial organizational psychology", "industrial organisational psychology",
                         #"industrial organizational psych", "industrial organisational psych"
                        )) %>% #removed variations of I/O psychology. Punctuation already removed previously so not needed here. removed because it trailed all & characters. 
  tm_map(stripWhitespace) %>%
  lemmatize_words() %>% #used a preset dictionary to homogenize word forms 
  tm_filter(FUN = function(x) { return(nchar(stripWhitespace(x$content)[[1]]) > 1) }) #filters 2 cases, comment out to find empty rows and rerun code chunk below

compare_them <- function(x, y) { 
  index <- sample(1:length(x),1) #set an index
  print(x[[index]]$content) #print content of index case x
  print(y[[index]]$content) #print content of index case y
  }
compare_them(io_corpus_original, io_corpus) #run it as many times you like for checks

myTokenizer <- function(x) { 
  NGramTokenizer(x, Weka_control(min=1, max=2)) } #set max to 2 to enable bigrams (2-word sequences). Bigrams include unigrams! 

io_dtm <- DocumentTermMatrix(io_corpus, control = list(tokenize = myTokenizer)) #create a DTM

# Check for empty values (0s)
# io_dtm$dimnames$Terms 
# test_matrix <- as.matrix(io_dtm)
# test_tbl <- tibble(count = rowSums(test_matrix), tidy(io_corpus_original))
# View(test_tbl) #check for 0 in terms - 2 missing
# # content(io_corpus_original[[455]])
# # content(io_corpus_original[[524]])

io_slim_dtm <- removeSparseTerms(io_dtm, .997) #2.1 n/k ratio (n cases = 729, k variables = 346)

```

```{r Analysis, include=TRUE}
# Identifying number of topics
local_cluster <- makeCluster(7)
registerDoParallel(local_cluster) #Run across multiple cores! Faster.
tuning <- FindTopicsNumber(io_dtm,
                           topics = seq(5, 15, by = 1), #originally set between 5 and 30 by 5, then 5 and 15 by 1, and then this final restricted range, looking for alignment across models
                           metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"), 
                           verbose = T,
                           control = list(seed = 50))
FindTopicsNumber_plot(tuning) #look for convergence between, similar to a PCA
stopCluster(local_cluster) 
registerDoSEQ() #closes clusters

#LDA with 8 topics, 7 or 9 would also be reasonable
lda_results <- LDA(io_dtm, 
                  k = 8,
                  method = "Gibbs",
                  control = list(seed = 25))

lda_betas <- tidy(lda_results, matrix="beta") %>% 
  group_by(topic) %>%
  slice_max(n = 10, beta) %>%
  arrange(topic, -beta)
lda_betas # probability that word(s) belong in a topic

lda_gammas <- tidy(lda_results, matrix="gamma") %>% 
  group_by(document) %>% 
  slice_max(n = 1, gamma, with_ties = FALSE) %>% #highest probability only, removes ties
  mutate(doc_id = as.numeric(document)) %>%
  arrange(doc_id)
lda_gammas # probabilities of document containing the topic, less than 729 will require creative table building

# Create a table to interpret results
topics_tbl <- tidy(io_corpus_original) %>%
  right_join(lda_gammas, by = join_by(id == document)) %>%
  mutate(original = text, probability = gamma) %>%
  select(doc_id, original, topic, probability)
``` 

``` {r Visualization, include = TRUE}
wordcloud_data = as.matrix(io_dtm)
wordCounts <- colSums(wordcloud_data)
wordNames <- colnames(wordcloud_data)
colorset <- brewer.pal(7, "Dark2")
wordcloud(
	  words = wordNames,
		freq = wordCounts,
		max.words = 50,
		min.freq=2,
		scale=c(2,.25), #made print scale smaller to fit in more windows. Would remove words if it didn't fit
		colors=colorset) 
```

``` {r Publication, include = TRUE} 
final_week12_tbl <- mutate(week12_tbl, doc_id = as.numeric(row.names(week12_tbl))) #added doc_id matching in multiple tibbles
final_tbl <- right_join(topics_tbl, final_week12_tbl, by = "doc_id") %>% #retains all case numbers, even missing values. Check title vs original column to ensure it worked correctly
  select(-original) %>% #2 missing titles in topics tibble
  mutate(topic = factor(topic, ordered = FALSE)) %>%
  rename(original = title) %>% #creating unordered factors for lm analysis later, renames titles column so all titles are present
  arrange(doc_id)

# Does topic predict upvotes?       
model <- lm(upvotes ~ topic, data = final_tbl)
summary(model)
```
